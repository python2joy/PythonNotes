# 第三部分 爬虫进阶与实践

环境准备

下载docker for Mac并安装

利用`brew install mitmproxy`安装mitmproxy。

mitmproxy的使用需要启动代理层：系统偏好设置>网络>高级>代理>勾选“网页代理（HTTP）”和“安全网页代理（HTPPS）“，配置安全网页代理服务器地址`127.0.0.1`,冒号后面填写`8080`。相当于mitmproxy通过监测本地的`127.0.0.1`端口来看到并实现我们电脑访问的细节请求。

mitmproxy的使用：

Q是回到上一级或退出程序。

点击Get会显示访问的具体信息

z是清空当前页面，

平时不使用的时候可以把两个代理给关掉。



常用的一些库

dealing： 标准库，比较基础

urllib3： 比urllib多了些fetcher，更好用一些

requests：put请求修改内容，post则不会。

hyper：底层库，支持http/2。http是访问网页最重要的协议，是一个短链接，要请求才能发东西。爬虫都要通过http协议，对网页进行分析，比如requests、response。http/2是一个更新的版本，支持了长链接，可以和客户端一直保持在线，推送数据。http/2是与移动互联网适配的协议。



pip3 install --upgrade pip



现在能爬的网站越来越少，互联网公司每周二或周四会进行一次更新。

爬虫就是模拟人的访问来抓取数据，分为以下五步：

* 抓取
* 解析
* 存储
* 反爬
* 效率

爬取分为两种策略：

* 网页
  * 网站网页：一般选择的方案。
  * H5：如果有h5，可以爬取h5，h5的弱点相对较多，能够较好地爬取。
* App：App不太好爬取，是最后的选择。



网页

服务端渲染：淘宝、微博的数据都是在服务端渲染并返回。服务端渲染比较大的特征是Ajax，可以做异步的事情，可以异步调用服务端接口。Ajax通过拉取sever的数据往里边做填充。



客户端渲染：所有的请求和拼接都是在客户端完成的。有时服务端渲染的数据没有，但是在客户端渲染能找到这些数据。

抓包：

抓包工具：Charles(Mac)、Finder(win)、mitmproxy、Chorme、Firefox。mitmproxy只要有python 环境就能用。



抓包示例：网易严选热销榜

网址：`https://you.163.com/item/saleRank`

找到Ajax内容：

检查 > network > XHR，找到相应的信息页面

点击response看返回了什么东西，通过一个一个name找，找到我们想要的response。response中要包含`item`，其代码内容基本上是网页的html，有网页上的所有文字内容。可以把代码复制粘贴到编辑器中进行进一步的区分。在网页中找到一个内容，通过搜索测试一下是否在粘贴的代码中。如此，就找到了API内容。

找到对应name下的headers，在general中复制粘贴网址`Request URL`。

一般调用API去调试的时候，代码内容杂乱无章，看起来很不方便。

引入工具postman，更好地调节接口，查看API的内容。把`Request URL`的内容粘贴到postman的get里，然后点击获取。会对API内容进行完美的格式化，具有很强的格式化特征。（在浏览器中安装JSON-handle插件，将`Request URL`作为网址进行链接，也会获取到结构化的API内容。）

可以看出，爬虫越来越变成一个攻防战。



模拟浏览器/无头浏览器请求

用模拟浏览器/无头浏览把html拿下来进行分析。

请求工具：

Selenium：本来是做测试的一个工具，爬虫的人拿来做爬虫了。

PhantomJS：脚本化的无头浏览器。无头浏览器是什么？

Splinter：包装类的东西

Spynner：支持Ajax，属于开源浏览器，比较轻量

pyppeter：python 版本的请求器，一些复杂的解析：比如很多Ajax

Splash：类似于模拟浏览器，常常与scrapy结合起来使用。

requests-html：requests的作者开发，和pyppeter进行了集成，对于html的解析更加人性化。应用性更高。

